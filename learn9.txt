learn_9.txt
DEEP LEARNING BASICS — EASY + INTERVIEW READY
============================================

PURPOSE
-------
This document is created for:
- Beginners in Deep Learning
- Data Scientist & ML Engineer interviews
- 365-day long-term understanding
- Smooth transition from ML to DL

Language: Very simple English  
Rule: Understand intuition before math

=================================================
SECTION 1: WHAT IS DEEP LEARNING?
=================================================

Q1. What is Deep Learning?
A:
Deep Learning is a part of Machine Learning that uses
Neural Networks with many layers to learn complex patterns.

-------------------------------------------------

Q2. Difference between ML and Deep Learning?
A:
Machine Learning:
- Needs feature engineering
- Works well on small data

Deep Learning:
- Learns features automatically
- Needs large data and compute

-------------------------------------------------

Q3. Where is Deep Learning used?
A:
- Image recognition
- Speech recognition
- Chatbots
- Recommendation systems
- Self-driving cars

=================================================
SECTION 2: NEURAL NETWORK BASICS
=================================================

Q4. What is a Neural Network?
A:
A Neural Network is inspired by the human brain.
It has neurons connected in layers.

-------------------------------------------------

Q5. Parts of a Neural Network?
A:
- Input layer
- Hidden layers
- Output layer

-------------------------------------------------

Q6. What is a neuron?
A:
A neuron:
- Takes inputs
- Applies weights
- Adds bias
- Passes through activation function

-------------------------------------------------

Q7. What is weight and bias?
A:
- Weight controls importance of input
- Bias shifts the output

=================================================
SECTION 3: ACTIVATION FUNCTIONS
=================================================

Q8. Why activation function is needed?
A:
Without activation function,
neural network becomes a simple linear model.

-------------------------------------------------

Q9. Common activation functions?
A:
- ReLU
- Sigmoid
- Tanh
- Softmax

-------------------------------------------------

Q10. What is ReLU?
A:
ReLU = max(0, x)
It helps model learn faster.

-------------------------------------------------

Q11. What is Sigmoid?
A:
Sigmoid converts values between 0 and 1.
Used in binary classification.

-------------------------------------------------

Q12. What is Softmax?
A:
Softmax converts outputs into probabilities.
Used in multi-class classification.

=================================================
SECTION 4: LOSS FUNCTIONS
=================================================

Q13. What is loss function in DL?
A:
Loss function measures how wrong the prediction is.

-------------------------------------------------

Q14. Common loss functions?
A:
- Mean Squared Error → Regression
- Binary Cross Entropy → Binary classification
- Categorical Cross Entropy → Multi-class

=================================================
SECTION 5: TRAINING A NEURAL NETWORK
=================================================

Q15. What is forward propagation?
A:
Data moves from input layer to output layer
to make prediction.

-------------------------------------------------

Q16. What is backpropagation?
A:
Backpropagation adjusts weights by propagating error backward.

-------------------------------------------------

Q17. What optimizer is used?
A:
Optimizers update weights to reduce loss.

Examples:
- Gradient Descent
- Adam
- RMSprop

-------------------------------------------------

Q18. Why Adam is popular?
A:
Adam adapts learning rate automatically
and converges faster.

=================================================
SECTION 6: OVERFITTING IN DEEP LEARNING
=================================================

Q19. What is overfitting in DL?
A:
Model learns training data too well
but fails on new data.

-------------------------------------------------

Q20. How to reduce overfitting?
A:
- More data
- Dropout
- Regularization
- Early stopping

-------------------------------------------------

Q21. What is Dropout?
A:
Dropout randomly turns off neurons
to prevent memorization.

=================================================
SECTION 7: COMMON DEEP LEARNING MODELS
=================================================

Q22. What is ANN?
A:
Artificial Neural Network for tabular data.

-------------------------------------------------

Q23. What is CNN?
A:
Convolutional Neural Network.
Used for images.

-------------------------------------------------

Q24. What is RNN?
A:
Recurrent Neural Network.
Used for sequence data.

-------------------------------------------------

Q25. What is LSTM?
A:
Long Short-Term Memory.
Solves long-term dependency problem in RNN.

-------------------------------------------------

Q26. What is Transformer?
A:
A model based on attention mechanism.
Used in NLP (ChatGPT type models).

=================================================
SECTION 8: DEEP LEARNING FRAMEWORKS
=================================================

Q27. Popular DL frameworks?
A:
- TensorFlow
- Keras
- PyTorch

-------------------------------------------------

Q28. Why Keras is beginner-friendly?
A:
Because it has simple and readable syntax.

=================================================
SECTION 9: DL IN INTERVIEWS
=================================================

Q29. Why DL needs more data?
A:
Because it has many parameters to learn.

-------------------------------------------------

Q30. Why GPU is used?
A:
Because DL involves heavy matrix computations.

-------------------------------------------------

Q31. Difference between Epoch and Batch?
A:
- Epoch → One full pass of data
- Batch → Small part of data

-------------------------------------------------

Q32. What is learning rate?
A:
Controls how big a step model takes while learning.

=================================================
SECTION 10: ML VS DL SUMMARY
=================================================

ML:
- Less data
- Feature engineering
- Faster training

DL:
- Large data
- Automatic feature learning
- High accuracy

=================================================
SECTION 11: 365-DAY DL MINDSET
=================================================

RULES:
- Start with ANN
- Understand basics before CNN/RNN
- Practice small models
- Do not rush advanced math

REMEMBER:
Deep Learning is powerful, but fundamentals matter.

=================================================
FINAL REVISION (ONE-LINERS)
=================================================

- Neural networks learn features
- ReLU speeds learning
- Backpropagation updates weights
- Adam optimizer is popular
- CNN → images
- RNN/LSTM → sequences
- Transformers → attention

=================================================
END OF learn_9.txt
=================================================
learn_4.txt
MACHINE LEARNING CODING — DEEP INTERVIEW PREPARATION (365 DAYS)
===============================================================

PURPOSE
-------
This document is for:
- Deep ML understanding
- Long-term learning (365 days)
- Coding + concept clarity
- Data Scientist & ML Engineer interviews

Language: Very easy English  
Approach: Slow → Deep → Practical  
Rule: Understand WHY before HOW

=================================================
SECTION 1: PYTHON FOR ML (MUST BE STRONG)
=================================================

Q1. Why Python is used in Machine Learning?
A:
Python is easy to read, easy to write, and has powerful libraries.
It allows faster experimentation.

Key ML libraries:
- NumPy
- Pandas
- Matplotlib
- Scikit-learn

-------------------------------------------------

Q2. Difference between list and NumPy array?
A:
List:
- Can store different data types
- Slower for calculations

NumPy array:
- Stores same data type
- Faster for math operations
- Used in ML

-------------------------------------------------

Q3. What is vectorization?
A:
Vectorization means performing operations on arrays without loops.
It makes code faster and efficient.

-------------------------------------------------

Q4. Why loops are avoided in ML?
A:
Loops are slow for large data.
ML uses matrix operations for speed.

=================================================
SECTION 2: DATA HANDLING & PREPROCESSING
=================================================

Q5. What is data preprocessing?
A:
Preparing raw data so that ML models can understand it.

Includes:
- Handling missing values
- Encoding categorical data
- Feature scaling
- Removing noise

-------------------------------------------------

Q6. How do you handle missing values?
A:
Common methods:
- Remove rows
- Fill with mean / median
- Fill with most frequent value

-------------------------------------------------

Q7. Why categorical encoding is needed?
A:
ML models work with numbers, not text.
So we convert text to numbers.

Examples:
- Label Encoding
- One-Hot Encoding

-------------------------------------------------

Q8. What is feature scaling?
A:
Making all features on the same scale so models work correctly.

-------------------------------------------------

Q9. When NOT to scale?
A:
Tree-based models do not need scaling.

=================================================
SECTION 3: TRAIN–TEST–VALIDATION LOGIC
=================================================

Q10. Why do we split data?
A:
To test model on unseen data and avoid overfitting.

-------------------------------------------------

Q11. What is validation set?
A:
Data used to tune model parameters.

Split example:
- Train: 70%
- Validation: 15%
- Test: 15%

-------------------------------------------------

Q12. What is data leakage?
A:
When test data information is used during training.
This gives false high accuracy.

=================================================
SECTION 4: LINEAR REGRESSION (CODING + THINKING)
=================================================

Q13. Why linear regression is important?
A:
It is the base of many ML algorithms.

-------------------------------------------------

Q14. What equation does linear regression use?
A:
y = wX + b

-------------------------------------------------

Q15. What is MSE?
A:
Mean Squared Error measures average squared error.

-------------------------------------------------

Q16. Why Gradient Descent is used?
A:
To find best weights that minimize loss.

-------------------------------------------------

Q17. What happens if learning rate is too high?
A:
Model will not converge and may diverge.

-------------------------------------------------

Q18. Why scaling is important for GD?
A:
Because gradient descent is sensitive to feature scale.

=================================================
SECTION 5: CLASSIFICATION (VERY IMPORTANT)
=================================================

Q19. Difference between regression and classification?
A:
Regression predicts numbers.
Classification predicts categories.

-------------------------------------------------

Q20. What is Logistic Regression?
A:
A classification algorithm that outputs probability.

-------------------------------------------------

Q21. Why sigmoid function is used?
A:
To convert output into range 0 to 1.

-------------------------------------------------

Q22. Why Logistic Regression is NOT regression?
A:
Because it predicts class, not continuous value.

=================================================
SECTION 6: DISTANCE-BASED MODELS
=================================================

Q23. What is KNN?
A:
A model that predicts based on nearest neighbors.

-------------------------------------------------

Q24. Why KNN is called lazy learner?
A:
Because it does not learn during training.
It learns during prediction.

-------------------------------------------------

Q25. Why scaling is mandatory for KNN?
A:
Because distance calculation is affected by feature size.

-------------------------------------------------

Q26. How to choose K?
A:
Using cross-validation and accuracy vs K.

=================================================
SECTION 7: SVM (HARD BUT POWERFUL)
=================================================

Q27. What is SVM?
A:
A model that finds the best boundary with maximum margin.

-------------------------------------------------

Q28. What are support vectors?
A:
The closest points to the decision boundary.

-------------------------------------------------

Q29. What is kernel trick?
A:
A method to handle non-linear data by transforming features.

-------------------------------------------------

Q30. Why SVM needs scaling?
A:
Because it uses distance and dot products.

=================================================
SECTION 8: TREE-BASED MODELS
=================================================

Q31. What is Decision Tree?
A:
A model that makes decisions using rules.

-------------------------------------------------

Q32. Why trees overfit?
A:
Because they memorize training data.

-------------------------------------------------

Q33. What is Random Forest?
A:
An ensemble of many decision trees.

-------------------------------------------------

Q34. Why Random Forest is better?
A:
It reduces variance and improves stability.

=================================================
SECTION 9: BOOSTING (ADVANCED)
=================================================

Q35. What is Boosting?
A:
Training models sequentially where each model fixes previous mistakes.

-------------------------------------------------

Q36. Difference between Bagging and Boosting?
A:
Bagging reduces variance.
Boosting reduces bias.

-------------------------------------------------

Q37. Why XGBoost is popular?
A:
Because it is fast, accurate, and uses regularization.

=================================================
SECTION 10: MODEL EVALUATION
=================================================

Q38. Why accuracy is not enough?
A:
Because it fails on imbalanced data.

-------------------------------------------------

Q39. What is precision?
A:
Correct positive predictions out of predicted positives.

-------------------------------------------------

Q40. What is recall?
A:
Correct positive predictions out of actual positives.

-------------------------------------------------

Q41. What is confusion matrix?
A:
A table showing prediction errors.

=================================================
SECTION 11: PIPELINES & DEPLOYMENT
=================================================

Q42. What is a pipeline?
A:
A sequence of preprocessing and model steps.

-------------------------------------------------

Q43. Why pipelines are important?
A:
They prevent data leakage.

-------------------------------------------------

Q44. What is model deployment?
A:
Making model usable in real applications.

-------------------------------------------------

Q45. What is .pkl file?
A:
A file that stores trained ML model.

=================================================
SECTION 12: 365-DAY MINDSET (VERY IMPORTANT)
=================================================

RULES:
- Learn daily (even 30 minutes)
- Code every concept
- Revise weekly
- Explain concepts aloud
- Build small projects

REMEMBER:
Understanding > Memorization

=================================================
FINAL REVISION LINES
=================================================

- ML is pattern learning
- Data quality matters most
- Simpler models first
- Scaling matters
- Overfitting is enemy
- Pipelines save careers

=================================================
END OF learn_4.txt
=================================================
